{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path as osp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "from attrdict import AttrDict\n",
    "\n",
    "from evaluation import make_fig, make_logger\n",
    "\n",
    "from data import load_data, tensors_from_data\n",
    "from mnist_model import SeqAIRonMNIST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "n_steps = 3\n",
    "\n",
    "results_dir = '../results/seq'\n",
    "run_name = 'test'\n",
    "\n",
    "logdir = osp.join(results_dir, run_name)\n",
    "checkpoint_name = osp.join(logdir, 'model.ckpt')\n",
    "axes = {'imgs': 0, 'labels': 0, 'nums': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "num_steps_prior = AttrDict(\n",
    "    anneal='exp',\n",
    "    init=1.,\n",
    "    final=1e-1,\n",
    "    steps_div=1e4,\n",
    "    steps=1e5,\n",
    "    hold_init=1e3,\n",
    "    analytic=False\n",
    ")\n",
    "# num_steps_prior = None\n",
    "\n",
    "appearance_prior = AttrDict(loc=0., scale=1.)\n",
    "where_scale_prior = AttrDict(loc=0., scale=1.)\n",
    "where_shift_prior = AttrDict(loc=0., scale=1.)\n",
    "\n",
    "use_reinforce = True\n",
    "sample_presence = True\n",
    "step_bias = .75\n",
    "transform_var_bias = .5\n",
    "output_multiplier = .5\n",
    "\n",
    "l2_weight = 0. #1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_data = load_data('mnist_validation.pickle')\n",
    "train_data = load_data('mnist_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_tensors = tensors_from_data(train_data, batch_size, axes, shuffle=True)\n",
    "valid_tensors = tensors_from_data(valid_data, batch_size, axes, shuffle=False)\n",
    "x, valid_x = train_tensors['imgs'], valid_tensors['imgs']\n",
    "y, test_y = train_tensors['nums'], valid_tensors['nums']\n",
    "    \n",
    "n_hiddens = 32 * 8\n",
    "n_layers = 2\n",
    "n_hiddens = [n_hiddens] * n_layers\n",
    "    \n",
    "seq_x = tf.reshape(x, (4, 16, 50, 50))\n",
    "offset = tf.placeholder(tf.int32, [], 'offset')\n",
    "offset = tf.Variable(0, dtype=tf.int32, name='offset', trainable=False)\n",
    "seq_x = seq_x[offset:]\n",
    "print seq_x.get_shape()\n",
    "seq_y = tf.reshape(y, (4, 16, -1))[offset:]\n",
    "air = SeqAIRonMNIST(seq_x,\n",
    "                max_steps=n_steps,\n",
    "                inpt_encoder_hidden=n_hiddens,\n",
    "                glimpse_encoder_hidden=n_hiddens,\n",
    "                glimpse_decoder_hidden=n_hiddens,\n",
    "                transform_estimator_hidden=n_hiddens,\n",
    "                steps_pred_hidden=[128, 64],\n",
    "                baseline_hidden=[256, 128],\n",
    "                transform_var_bias=transform_var_bias,\n",
    "                step_bias=step_bias,\n",
    "                output_multiplier=output_multiplier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() (?, 16) (?, 16) (?, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "train_step, global_step = air.train_step(learning_rate, l2_weight, appearance_prior, where_scale_prior,\n",
    "                            where_shift_prior, num_steps_prior, use_reinforce=use_reinforce,\n",
    "                            decay_rate=None, nums=seq_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqAIRonMNIST/lstm_initial_state_0/w:0 (1, 256)\n",
      "SeqAIRonMNIST/lstm_initial_state_1/w:0 (1, 256)\n",
      "SeqAIRonMNIST/SeqAIRCell/Encoder/MLP/linear/w:0 (2500, 256)\n",
      "SeqAIRonMNIST/SeqAIRCell/Encoder/MLP/linear/b:0 (256,)\n",
      "SeqAIRonMNIST/SeqAIRCell/Encoder/MLP/linear_1/w:0 (256, 256)\n",
      "SeqAIRonMNIST/SeqAIRCell/Encoder/MLP/linear_1/b:0 (256,)\n",
      "lstm/w_gates:0 (512, 1024)\n",
      "lstm/b_gates:0 (1024,)\n",
      "SeqAIRonMNIST/SeqAIRCell/StochasticTransformParam/MLP/linear/w:0 (256, 256)\n",
      "SeqAIRonMNIST/SeqAIRCell/StochasticTransformParam/MLP/linear/b:0 (256,)\n",
      "SeqAIRonMNIST/SeqAIRCell/StochasticTransformParam/MLP/linear_1/w:0 (256, 256)\n",
      "SeqAIRonMNIST/SeqAIRCell/StochasticTransformParam/MLP/linear_1/b:0 (256,)\n",
      "SeqAIRonMNIST/SeqAIRCell/StochasticTransformParam/MLP/linear_2/w:0 (256, 8)\n",
      "SeqAIRonMNIST/SeqAIRCell/StochasticTransformParam/MLP/linear_2/b:0 (8,)\n",
      "SeqAIRonMNIST/SeqAIRCell/StepsPredictor/MLP/linear/w:0 (256, 128)\n",
      "SeqAIRonMNIST/SeqAIRCell/StepsPredictor/MLP/linear/b:0 (128,)\n",
      "SeqAIRonMNIST/SeqAIRCell/StepsPredictor/MLP/linear_1/w:0 (128, 64)\n",
      "SeqAIRonMNIST/SeqAIRCell/StepsPredictor/MLP/linear_1/b:0 (64,)\n",
      "SeqAIRonMNIST/SeqAIRCell/StepsPredictor/MLP/linear_2/w:0 (64, 1)\n",
      "SeqAIRonMNIST/SeqAIRCell/StepsPredictor/MLP/linear_2/b:0 (1,)\n",
      "SeqAIRonMNIST/SeqAIRCell/Encoder_1/MLP/linear/w:0 (400, 256)\n",
      "SeqAIRonMNIST/SeqAIRCell/Encoder_1/MLP/linear/b:0 (256,)\n",
      "SeqAIRonMNIST/SeqAIRCell/Encoder_1/MLP/linear_1/w:0 (256, 256)\n",
      "SeqAIRonMNIST/SeqAIRCell/Encoder_1/MLP/linear_1/b:0 (256,)\n",
      "SeqAIRonMNIST/SeqAIRCell/ParametrisedGaussian/linear/w:0 (256, 100)\n",
      "SeqAIRonMNIST/SeqAIRCell/ParametrisedGaussian/linear/b:0 (100,)\n",
      "SeqAIRonMNIST/SeqAIRCell/Decoder/MLP/linear/w:0 (50, 256)\n",
      "SeqAIRonMNIST/SeqAIRCell/Decoder/MLP/linear/b:0 (256,)\n",
      "SeqAIRonMNIST/SeqAIRCell/Decoder/MLP/linear_1/w:0 (256, 256)\n",
      "SeqAIRonMNIST/SeqAIRCell/Decoder/MLP/linear_1/b:0 (256,)\n",
      "SeqAIRonMNIST/SeqAIRCell/Decoder/MLP/linear_2/w:0 (256, 400)\n",
      "SeqAIRonMNIST/SeqAIRCell/Decoder/MLP/linear_2/b:0 (400,)\n",
      "BaselineMLP/MLP/linear/w:0 (3177, 256)\n",
      "BaselineMLP/MLP/linear/b:0 (256,)\n",
      "BaselineMLP/MLP/linear_1/w:0 (256, 128)\n",
      "BaselineMLP/MLP/linear_1/b:0 (128,)\n",
      "BaselineMLP/MLP/linear_2/w:0 (128, 1)\n",
      "BaselineMLP/MLP/linear_2/b:0 (1,)\n"
     ]
    }
   ],
   "source": [
    "for v in tf.trainable_variables():\n",
    "    print v.name, v.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "    \n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step_prob = air.num_steps_distrib.prob()\n",
    "# mean_prob = tf.reduce_mean(step_prob, 0)\n",
    "# # step_entropy = step_prob * tf.log(step_prob)\n",
    "# # step_entropy = -tf.reduce_sum(step_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def compute_grad(y, x):\n",
    "#     g = tf.reduce_mean(tf.gradients(y, x)[0], 0)\n",
    "#     norm = 1.#tf.sqrt(tf.reduce_sum(tf.square(g), -1, keep_dims=True))\n",
    "#     return g / norm\n",
    "\n",
    "# total_grad = compute_grad(air.opt_loss, step_prob)\n",
    "# kl_grad = compute_grad(air.prior_loss.value, step_prob)\n",
    "\n",
    "# grads = [total_grad, kl_grad]\n",
    "# names = ['total', 'kl']\n",
    "\n",
    "# if use_reinforce:\n",
    "#     reinforce_grad = compute_grad(air.reinforce_loss, step_prob)\n",
    "#     grads.append(reinforce_grad)\n",
    "#     names.append('reinforce')\n",
    "    \n",
    "# grads.append(mean_prob)\n",
    "# names.append('prob')\n",
    "\n",
    "\n",
    "# for g, name in zip(grads, names):\n",
    "#     step_grads = tf.unstack(g, axis=-1)\n",
    "#     for i, step_grad in enumerate(step_grads):\n",
    "#         tf.summary.scalar('prob_grad/{}_{}'.format(name, i), step_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print grads\n",
    "# print names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a, d = sess.run([grads, mean_prob])\n",
    "# for i in a:\n",
    "#     print i\n",
    "# print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_summaries = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_batches = train_data['imgs'].shape[0] // batch_size\n",
    "valid_batches = valid_data['imgs'].shape[0] // batch_size\n",
    "log = make_logger(air, sess, summary_writer, train_tensors, train_batches, valid_tensors, valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensors = {\n",
    "#     'loss': air.loss.value,\n",
    "#     'imp_weight': .5*tf.reduce_mean((air.reinforce_imp_weight)**2),\n",
    "#     'baseline_loss': air.baseline_loss,\n",
    "#     'baseline': tf.reduce_mean(air.baseline)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output = sess.run(tensors)\n",
    "# for o, v in output.iteritems():\n",
    "#     print '{}: {}'.format(o, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "* add sequential air model and it's MNISt version\n",
    "* adjust STN and BaselineMLP to work with timeseries\n",
    "* move anneal_weight fun to ops.py\n",
    "* implement 'prob' method of NumStepsDistribution for \n",
    "* add 'opt_loss' to logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at iter = 0\n",
      "Step 0, Data train prior_loss = 33.9371, loss = -66.1596, baseline_loss = 110608.9784, kl_what = 0.3690, imp_weight = -67.3129, opt_loss = 24.3343, kl_num_steps = 33.1958, reinforce_loss = 90.4939, rec_loss = -100.0966, num_step = 1.4717, num_step_acc = 0.2478, kl_where = 0.3724, eval time = 1.691s\n",
      "Step 0, Data test prior_loss = 33.9803, loss = 5.4206, baseline_loss = 106089.4497, kl_what = 0.4070, imp_weight = 4.1189, opt_loss = -0.1373, kl_num_steps = 33.1956, reinforce_loss = -5.5579, rec_loss = -28.5596, num_step = 1.4965, num_step_acc = 0.2257, kl_where = 0.3777, eval time = 0.4598s\n",
      "\n",
      "Step 1000, Data train prior_loss = 65.4491, loss = -338.1859, baseline_loss = 2496.4797, kl_what = 21.1333, imp_weight = -17.4861, opt_loss = -314.7169, kl_num_steps = 33.1946, reinforce_loss = 23.4691, rec_loss = -403.6351, num_step = 1.2769, num_step_acc = 0.2818, kl_where = 11.1213, eval time = 1.634s\n",
      "Step 1000, Data test prior_loss = 68.2813, loss = -281.3978, baseline_loss = 2058.2043, kl_what = 23.7677, imp_weight = -18.2863, opt_loss = -256.7753, kl_num_steps = 33.1900, reinforce_loss = 24.6224, rec_loss = -349.6790, num_step = 1.3090, num_step_acc = 0.2743, kl_where = 11.3236, eval time = 0.2696s\n",
      "\n",
      "Step 2000, Data train prior_loss = 44.3169, loss = -394.5061, baseline_loss = 867.3421, kl_what = 26.6748, imp_weight = -1.2685, opt_loss = -392.7917, kl_num_steps = 2.4553, reinforce_loss = 1.7144, rec_loss = -438.8230, num_step = 1.3564, num_step_acc = 0.2543, kl_where = 15.1868, eval time = 1.656s\n",
      "Step 2000, Data test prior_loss = 48.0138, loss = -347.1822, baseline_loss = 960.7116, kl_what = 30.0937, imp_weight = -1.1170, opt_loss = -345.6729, kl_num_steps = 2.4527, reinforce_loss = 1.5094, rec_loss = -395.1960, num_step = 1.3281, num_step_acc = 0.2413, kl_where = 15.4673, eval time = 0.2649s\n",
      "\n",
      "Step 3000, Data train prior_loss = 39.3197, loss = -416.0495, baseline_loss = 626.4542, kl_what = 26.1176, imp_weight = 5.2360, opt_loss = -423.2073, kl_num_steps = 1.8032, reinforce_loss = -7.1578, rec_loss = -455.3692, num_step = 1.3516, num_step_acc = 0.2702, kl_where = 11.3989, eval time = 1.709s\n",
      "Step 3000, Data test prior_loss = 41.5410, loss = -357.3663, baseline_loss = 824.3276, kl_what = 28.2280, imp_weight = 8.5397, opt_loss = -368.7942, kl_num_steps = 1.8040, reinforce_loss = -11.4279, rec_loss = -398.9073, num_step = 1.2656, num_step_acc = 0.2500, kl_where = 11.5089, eval time = 0.2677s\n",
      "\n",
      "Step 4000, Data train prior_loss = 41.0051, loss = -420.8392, baseline_loss = 525.1401, kl_what = 28.1796, imp_weight = 7.9617, opt_loss = -431.7129, kl_num_steps = 1.4435, reinforce_loss = -10.8737, rec_loss = -461.8443, num_step = 1.4652, num_step_acc = 0.2505, kl_where = 11.3820, eval time = 1.672s\n",
      "Step 4000, Data test prior_loss = 45.7850, loss = -373.2239, baseline_loss = 765.2122, kl_what = 32.2132, imp_weight = 10.9381, opt_loss = -388.0829, kl_num_steps = 1.4426, reinforce_loss = -14.8590, rec_loss = -419.0089, num_step = 1.4965, num_step_acc = 0.2378, kl_where = 12.1292, eval time = 0.2679s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_itr = sess.run(global_step)\n",
    "print 'Starting training at iter = {}'.format(train_itr)\n",
    "\n",
    "if train_itr == 0:\n",
    "    sess.run(air._baseline_tran_step)\n",
    "    log(0)\n",
    "    \n",
    "while train_itr < 1e6:\n",
    "        \n",
    "    train_itr, _ = sess.run([global_step, train_step])\n",
    "    \n",
    "#     if train_itr % 100 == 0:\n",
    "#     if (train_itr % 1000) < 100:\n",
    "    if train_itr % 50 == 0:\n",
    "        summaries = sess.run(all_summaries)\n",
    "        summary_writer.add_summary(summaries, train_itr)\n",
    "        \n",
    "    if train_itr % 1000 == 0:\n",
    "        log(train_itr)\n",
    "        \n",
    "    if train_itr % 5000 == 0:\n",
    "        saver.save(sess, checkpoint_name, global_step=train_itr)\n",
    "#         make_fig(air, sess, logdir, train_itr)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make_fig(air, sess, n_samples=64)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
