{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path as osp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "\n",
    "from tensorflow.contrib.distributions import Bernoulli, NormalWithSoftplusScale, Normal\n",
    "from tensorflow.contrib.distributions.python.ops.kullback_leibler import kl as _kl\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from neurocity import minimize_clipped\n",
    "from neurocity.tools.params import num_trainable_params\n",
    "\n",
    "from tf_tools.eval import make_expr_logger, gradient_summaries\n",
    "\n",
    "from data import load_data, tensors_from_data\n",
    "from model import AIRCell\n",
    "from ops import Loss\n",
    "from prior import geometric_prior, presence_prob_table, tabular_kl, NumStepsDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "img_size = 50, 50\n",
    "crop_size = 20, 20\n",
    "n_latent = 50\n",
    "n_hidden = 256\n",
    "n_steps = 3\n",
    "\n",
    "results_dir = '../results'\n",
    "run_name = 'test'\n",
    "\n",
    "logdir = osp.join(results_dir, run_name)\n",
    "checkpoint_name = osp.join(logdir, 'model.ckpt')\n",
    "axes = {'imgs': 0, 'labels': 0, 'nums': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior_weight = .3 ** 2\n",
    "\n",
    "num_steps_prior = 5e-2\n",
    "latent_code_prior = dict(loc=0., scale=1.)\n",
    "\n",
    "use_reinforce = True\n",
    "sample_presence = True\n",
    "presence_bias = 0.\n",
    "\n",
    "init_explore_eps = .05\n",
    "\n",
    "l2_weight = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = load_data('mnist_test.pickle')\n",
    "train_data = load_data('mnist_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_tensors = tensors_from_data(train_data, batch_size, axes, shuffle=True)\n",
    "test_tensors = tensors_from_data(test_data, batch_size, axes, shuffle=False)\n",
    "x, test_x = train_tensors['imgs'], test_tensors['imgs']\n",
    "y, test_y = train_tensors['nums'], test_tensors['nums']\n",
    "\n",
    "if init_explore_eps > 0:\n",
    "    explore_eps = tf.get_variable('explore_eps', initializer=init_explore_eps, trainable=False)\n",
    "else:\n",
    "    explore_eps = None\n",
    "    \n",
    "transition = snt.LSTM(n_hidden)\n",
    "air = AIRCell(img_size, crop_size, n_latent, transition, max_crop_size=1.0,\n",
    "              canvas_init=None,\n",
    "              sample_presence=sample_presence,\n",
    "              presence_bias=presence_bias,\n",
    "              explore_eps=explore_eps,\n",
    "              debug=True)\n",
    "\n",
    "initial_state = air.initial_state(x)\n",
    "\n",
    "dummy_sequence = tf.zeros((n_steps, batch_size, 1), name='dummy_sequence')\n",
    "outputs, state = tf.nn.dynamic_rnn(air, dummy_sequence, initial_state=initial_state, time_major=True)\n",
    "canvas, cropped, what, what_loc, what_scale, where, presence_prob, presence = outputs\n",
    "\n",
    "with tf.variable_scope('notebook'):\n",
    "    cropped = tf.reshape(presence * tf.nn.sigmoid(cropped), (n_steps, batch_size,) + tuple(crop_size))\n",
    "    canvas = tf.reshape(canvas, (n_steps, batch_size,) + tuple(img_size))\n",
    "    final_canvas = canvas[-1]\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('baseline'):\n",
    "    constant_baseline = snt.TrainableVariable([], initializers={'w': tf.zeros_initializer()}, name='constant_baseline')\n",
    "    parts = [tf.reshape(tf.transpose(i, (1, 0, 2)), (batch_size, -1)) for i in (what, where, presence_prob)]\n",
    "    img_flat = tf.reshape(x, (batch_size, -1))\n",
    "    baseline_inpts = [img_flat] + parts\n",
    "    baseline_inpts = tf.concat(baseline_inpts, -1)\n",
    "    \n",
    "    lin1 = snt.Linear(100)\n",
    "    lin2 = snt.Linear(1)\n",
    "    seq = snt.Sequential([lin1, tf.nn.elu, lin2])\n",
    "    baseline = seq(baseline_inpts) + constant_baseline()\n",
    "    \n",
    "    baseline_mean = tf.reduce_mean(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1262283.0\n"
     ]
    }
   ],
   "source": [
    "print num_trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###    Loss #################################################################################\n",
    "loss = Loss()\n",
    "prior_loss = Loss()\n",
    "train_step = []\n",
    "lr_tensor = tf.Variable(learning_rate, name='learning_rate', trainable=False)\n",
    "\n",
    "###    Reconstruction Loss ##################################################################\n",
    "# rec_loss_per_sample = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=final_canvas)\n",
    "rec_loss_per_sample = (x - final_canvas) ** 2\n",
    "\n",
    "rec_loss_per_sample = tf.reduce_sum(rec_loss_per_sample, axis=(1, 2))\n",
    "rec_loss = tf.reduce_mean(rec_loss_per_sample)\n",
    "tf.summary.scalar('rec_loss', rec_loss)\n",
    "\n",
    "loss.add(rec_loss, rec_loss_per_sample)\n",
    "# # ###    Prior Loss ###########################################################################\n",
    "\n",
    "num_step_per_sample = tf.to_float(tf.squeeze(tf.reduce_sum(presence, 0)))\n",
    "num_step = tf.reduce_mean(num_step_per_sample)\n",
    "if prior_weight > 0.:\n",
    "    \n",
    "    if num_steps_prior is not None:  \n",
    "        prior = geometric_prior(num_steps_prior, 3)\n",
    "        posterior_probs = tf.transpose(tf.squeeze(presence_prob))\n",
    "        presence_tabular_distrib = presence_prob_table(posterior_probs)\n",
    "        \n",
    "        steps_kl = tabular_kl(presence_tabular_distrib, prior)\n",
    "        num_steps_prior_loss_per_sample = tf.squeeze(tf.reduce_sum(steps_kl, 1))\n",
    "\n",
    "        num_steps_prior_loss = tf.reduce_mean(num_steps_prior_loss_per_sample)\n",
    "        tf.summary.scalar('num_steps_prior_loss', num_steps_prior_loss)\n",
    "        prior_loss.add(num_steps_prior_loss, num_steps_prior_loss_per_sample)\n",
    "\n",
    "    if latent_code_prior is not None:        \n",
    "        prior = Normal(latent_code_prior['loc'], latent_code_prior['scale'])\n",
    "        posterior = NormalWithSoftplusScale(what_loc, what_scale)\n",
    "        \n",
    "        what_kl = _kl(posterior, prior)\n",
    "        what_kl = tf.reduce_sum(what_kl, -1, keep_dims=True) * presence\n",
    "        latent_code_prior_loss_per_sample = tf.squeeze(tf.reduce_sum(what_kl, 0))\n",
    "    \n",
    "        n_samples_with_encoding = tf.reduce_sum(tf.to_float(tf.greater(num_step_per_sample, 0.)))\n",
    "        div = tf.maximum(n_samples_with_encoding, 1.)\n",
    "        latent_code_prior_loss = tf.reduce_sum(latent_code_prior_loss_per_sample) / div\n",
    "        tf.summary.scalar('latent_code_prior_loss', latent_code_prior_loss)\n",
    "        prior_loss.add(latent_code_prior_loss, latent_code_prior_loss_per_sample)\n",
    "\n",
    "    tf.summary.scalar('prior_loss', prior_loss.value)\n",
    "    loss.add(prior_loss, weight=prior_weight)\n",
    "\n",
    "# ###   REINFORCE ############################################################################\n",
    "\n",
    "opt_loss = loss.value\n",
    "if use_reinforce:\n",
    "#     clipped_presence_prob = tf.clip_by_value(presence_prob, 1e-7, 1. - 1e-7)\n",
    "#     log_prob = Bernoulli(probs=clipped_presence_prob).log_prob(presence)\n",
    "#     log_prob = tf.squeeze(tf.reduce_mean(log_prob, 0))\n",
    "\n",
    "#     clipped_presence_prob = tf.clip_by_value(posterior_probs, 1e-7, 1. - 1e-7)\n",
    "#     num_steps_distrib = NumStepsDistribution(clipped_presence_prob)\n",
    "    num_steps_distrib = NumStepsDistribution(posterior_probs)\n",
    "    log_prob = num_steps_distrib.log_prob(num_step_per_sample)\n",
    "    log_prob = tf.clip_by_value(log_prob, -1e32, 1e32)\n",
    "\n",
    "    \n",
    "#     log_prob *= -1 # cause we're maximising\n",
    "\n",
    "    importance_weight = loss._per_sample\n",
    "    importance_weight -= baseline\n",
    "\n",
    "    reinforce_loss_per_sample = tf.stop_gradient(importance_weight) * log_prob\n",
    "    reinforce_loss = tf.reduce_mean(reinforce_loss_per_sample)\n",
    "    tf.summary.scalar('reinforce_loss', reinforce_loss)\n",
    "\n",
    "    opt_loss += reinforce_loss\n",
    "    \n",
    "    ### Baseline Optimisation ##################################################################################\n",
    "    baseline_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='baseline')\n",
    "    baseline_target = tf.stop_gradient(loss.per_sample)\n",
    "    baseline_loss_per_sample = (baseline_target - baseline) ** 2\n",
    "    baseline_loss = tf.reduce_mean(baseline_loss_per_sample)\n",
    "    tf.summary.scalar('baseline_loss', baseline_loss)\n",
    "\n",
    "    baseline_opt = tf.train.RMSPropOptimizer(10 * lr_tensor, momentum=.9, centered=True)\n",
    "    baseline_train_step = baseline_opt.minimize(baseline_loss, var_list=baseline_vars)\n",
    "    train_step.append(baseline_train_step)\n",
    "    \n",
    "    \n",
    "    \n",
    "###    Optimizer #################################################################################\n",
    "\n",
    "model_vars = list(set(tf.trainable_variables()) - set(baseline_vars))\n",
    "    \n",
    "#######    L2 reg ################################################################################\n",
    "if l2_weight > 0.:\n",
    "    l2_loss = l2_weight * sum(map(tf.nn.l2_loss, model_vars))\n",
    "    opt_loss += l2_loss\n",
    "    tf.summary.scalar('l2_loss', l2_loss)\n",
    "\n",
    "opt = tf.train.RMSPropOptimizer(lr_tensor, momentum=.9, centered=True)\n",
    "# true_train_step = opt.minimize(opt_loss)\n",
    "# true_train_step = minimize_clipped(opt, opt_loss, clip_value=.3, normalize_by_num_params=True)\n",
    "gvs = opt.compute_gradients(opt_loss, var_list=model_vars)\n",
    "true_train_step = opt.apply_gradients(gvs)\n",
    "train_step.append(true_train_step)\n",
    "\n",
    "gradient_summaries(gvs)\n",
    "\n",
    "\n",
    "###    Metrics #################################################################################\n",
    "gt_num = tf.squeeze(tf.reduce_sum(y, 0))\n",
    "num_step_accuracy = tf.reduce_mean(tf.to_float(tf.equal(gt_num, num_step_per_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "print what_kl.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping baseline/linear/w:0\n",
      "Skipping baseline/linear/b:0\n",
      "Skipping baseline/linear_1/w:0\n",
      "Skipping baseline/linear_1/b:0\n",
      "Skipping baseline/constant_baseline/w:0\n"
     ]
    }
   ],
   "source": [
    "vs = tf.trainable_variables()\n",
    "gs = tf.gradients(opt_loss, vs)\n",
    "\n",
    "for v, g in zip(vs, gs):\n",
    "    if g is None:\n",
    "        print 'Skipping', v.name\n",
    "    else:\n",
    "        assert v.get_shape() == g.get_shape(), v.name\n",
    "\n",
    "named_grads = {v.name: g for v, g in zip(vs, gs) if g is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_variance(n=10, sort_by_var=True):\n",
    "    gs = {k: [] for k in named_grads}\n",
    "    for i in xrange(n):\n",
    "        values = sess.run(named_grads)\n",
    "        for k, v in values.iteritems():\n",
    "            gs[k].append(v)\n",
    "\n",
    "    for k, v in gs.iteritems():\n",
    "        v = np.stack(v, 0).reshape((n, -1))\n",
    "        gs[k] = np.var(v, 0).mean()\n",
    "        \n",
    "    sort_idx = 1 if sort_by_var else 0\n",
    "    gs = sorted(gs.items(), key=lambda x: x[sort_idx], reverse=True)\n",
    "    return gs\n",
    "\n",
    "def print_grad_variance():\n",
    "    grad_vars = grad_variance(10)\n",
    "    print\n",
    "    for g in grad_vars:\n",
    "        if g[1] > 1e-2:\n",
    "            print g\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "    \n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# restore_dir = '/Users/adam/code/attend_infer_repeat/results/galactus/fixed_test_distrib_exp_5e-2'\n",
    "# restore_step = 1590000\n",
    "# restore_path = os.path.join(restore_dir, 'model.ckpt-{}'.format(restore_step))\n",
    "# saver.restore(sess, restore_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs = train_data['imgs']\n",
    "presence_gt = train_data['nums']\n",
    "train_itr = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "def rect(bbox, c=None, facecolor='none', label=None, ax=None):\n",
    "    r = Rectangle((bbox[1], bbox[0]), bbox[3], bbox[2],\n",
    "            edgecolor=c, facecolor=facecolor, label=label)\n",
    "\n",
    "    if ax is not None:\n",
    "        ax.add_patch(r)\n",
    "    return r\n",
    "\n",
    "def rect_stn(ax, width, height, w, c=None):\n",
    "    sx, tx, sy, ty = w\n",
    "    x = width * (1. - sx + tx) / 2\n",
    "    y = height * (1. - sy + ty) / 2\n",
    "    bbox = [y-.5, x-.5, height*sy, width*sx]\n",
    "    rect(bbox, c, ax=ax)\n",
    "\n",
    "\n",
    "def make_fig(checkpoint_dir=None, global_step=None):\n",
    "    xx, pred_canvas, pred_crop, prob, pres, w = sess.run([x, canvas, cropped, presence_tabular_distrib[..., 1:], presence, where])\n",
    "    height, width = xx.shape[1:]\n",
    "    \n",
    "    max_imgs = 10\n",
    "    bs = min(max_imgs, batch_size)\n",
    "    scale = 1.5\n",
    "    figsize = scale * np.asarray((bs, 2 * n_steps + 1))\n",
    "    fig, axes = plt.subplots(2 * n_steps + 1, bs, figsize=figsize)\n",
    "\n",
    "    for i, ax in enumerate(axes[0]):\n",
    "        ax.imshow(xx[i], cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "    for i, ax_row in enumerate(axes[1:1+n_steps]):\n",
    "        for j, ax in enumerate(ax_row):\n",
    "            ax.imshow(pred_canvas[i, j], cmap='gray', vmin=0, vmax=1)\n",
    "            if pres[i, j, 0] > .5:\n",
    "                rect_stn(ax, width, height, w[i, j], 'r')\n",
    "\n",
    "    for i, ax_row in enumerate(axes[1+n_steps:]):\n",
    "        for j, ax in enumerate(ax_row):\n",
    "            ax.imshow(pred_crop[i, j], cmap='gray')#, vmin=0, vmax=1)\n",
    "            ax.set_title('{:d} with p({:d}) = {:.02f}'.format(int(pres[i, j, 0]), i+1, prob[j, i].squeeze()), fontsize=4*scale)\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "    \n",
    "    if checkpoint_dir is not None:\n",
    "        fig_name = osp.join(checkpoint_dir, 'progress_fig_{}.png'.format(global_step))\n",
    "        fig.savefig(fig_name, dpi=300)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exprs = {\n",
    "    'loss': loss.value,\n",
    "    'rec_loss': rec_loss,\n",
    "    'num_step_acc': num_step_accuracy,\n",
    "    'num_step': num_step\n",
    "}\n",
    "\n",
    "if prior_weight > 0:\n",
    "    exprs['prior_loss'] = prior_loss.value\n",
    "    if num_steps_prior is not None:    \n",
    "        exprs['num_steps_prior_loss'] = num_steps_prior_loss\n",
    "\n",
    "    if latent_code_prior is not None:\n",
    "        exprs['latent_code_prior_loss'] = latent_code_prior_loss\n",
    "\n",
    "if use_reinforce:\n",
    "    exprs['baseline_loss'] = baseline_loss\n",
    "    exprs['reinforce_loss'] = reinforce_loss\n",
    "    exprs['imp_weight'] = tf.reduce_mean(importance_weight)\n",
    "    \n",
    "if l2_weight > 0:\n",
    "    exprs['l2_loss'] = l2_loss\n",
    "    \n",
    "train_log = make_expr_logger(sess, summary_writer, train_data['imgs'].shape[0] / batch_size, exprs, name='train')\n",
    "test_log = make_expr_logger(sess, summary_writer, test_data['imgs'].shape[0] / batch_size, exprs, name='test', data_dict={x: test_x, y: test_y})\n",
    "\n",
    "def log(train_itr):\n",
    "    train_log(train_itr)\n",
    "    test_log(train_itr)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at iter = -1\n",
      "Step 0, Data train prior_loss = 12.2799, loss = 133.9840, latent_code_prior_loss = 10.8647, imp_weight = 133.0639, num_steps_prior_loss = 1.4152, l2_loss = 0.0112, baseline_loss = 35028.7766, reinforce_loss = -225.2465, rec_loss = 132.8788, num_step = 0.8572, num_step_acc = 0.2970, eval time = 19.59s\n",
      "Step 0, Data test prior_loss = 11.7090, loss = 128.6124, latent_code_prior_loss = 10.2970, imp_weight = 127.7396, num_steps_prior_loss = 1.4120, l2_loss = 0.0112, baseline_loss = 33670.3812, reinforce_loss = -215.1491, rec_loss = 127.5585, num_step = 0.8208, num_step_acc = 0.2990, eval time = 0.4478s\n",
      "\n",
      "Step 1000, Data train prior_loss = 19.6664, loss = 35.2091, latent_code_prior_loss = 15.1145, imp_weight = 1.5259, num_steps_prior_loss = 4.5518, l2_loss = 0.0061, baseline_loss = 1007.8166, reinforce_loss = -7.7780, rec_loss = 33.4391, num_step = 1.6941, num_step_acc = 0.3621, eval time = 19.4s\n",
      "Step 1000, Data test prior_loss = 19.5869, loss = 33.5238, latent_code_prior_loss = 15.2188, imp_weight = -0.1497, num_steps_prior_loss = 4.3681, l2_loss = 0.0061, baseline_loss = 902.1567, reinforce_loss = -5.7729, rec_loss = 31.7610, num_step = 1.6115, num_step_acc = 0.3625, eval time = 0.3252s\n",
      "\n",
      "Step 2000, Data train prior_loss = 29.9075, loss = 30.4825, latent_code_prior_loss = 25.0279, imp_weight = -1.1073, num_steps_prior_loss = 4.8796, l2_loss = 0.0243, baseline_loss = 726.0836, reinforce_loss = -5.1738, rec_loss = 27.7909, num_step = 1.7686, num_step_acc = 0.3504, eval time = 20.82s\n",
      "Step 2000, Data test prior_loss = 29.5223, loss = 28.4759, latent_code_prior_loss = 24.6843, imp_weight = -3.0852, num_steps_prior_loss = 4.8380, l2_loss = 0.0243, baseline_loss = 649.7092, reinforce_loss = -3.2148, rec_loss = 25.8189, num_step = 1.7802, num_step_acc = 0.3594, eval time = 0.3395s\n",
      "\n",
      "Step 3000, Data train prior_loss = 41.1739, loss = 26.2776, latent_code_prior_loss = 36.1763, imp_weight = -0.7249, num_steps_prior_loss = 4.9975, l2_loss = 0.0668, baseline_loss = 537.0017, reinforce_loss = -5.6555, rec_loss = 22.5719, num_step = 1.8102, num_step_acc = 0.3534, eval time = 20.17s\n",
      "Step 3000, Data test prior_loss = 40.4921, loss = 24.7412, latent_code_prior_loss = 35.5335, imp_weight = -2.2255, num_steps_prior_loss = 4.9587, l2_loss = 0.0668, baseline_loss = 486.9160, reinforce_loss = -3.8272, rec_loss = 21.0969, num_step = 1.8344, num_step_acc = 0.3604, eval time = 0.3385s\n",
      "\n",
      "Step 4000, Data train prior_loss = 49.5290, loss = 24.2611, latent_code_prior_loss = 41.9720, imp_weight = 1.1201, num_steps_prior_loss = 7.5570, l2_loss = 0.0906, baseline_loss = 423.0115, reinforce_loss = -3.7228, rec_loss = 19.8035, num_step = 2.7028, num_step_acc = 0.0491, eval time = 19.74s\n",
      "Step 4000, Data test prior_loss = 47.9757, loss = 22.7370, latent_code_prior_loss = 40.4187, imp_weight = -0.3780, num_steps_prior_loss = 7.5570, l2_loss = 0.0906, baseline_loss = 369.9675, reinforce_loss = -1.7150, rec_loss = 18.4192, num_step = 2.7302, num_step_acc = 0.0396, eval time = 0.3249s\n",
      "\n",
      "Step 5000, Data train prior_loss = 46.7096, loss = 23.7820, latent_code_prior_loss = 41.7863, imp_weight = 1.3326, num_steps_prior_loss = 4.9233, l2_loss = 0.1057, baseline_loss = 457.9265, reinforce_loss = -7.1542, rec_loss = 19.5782, num_step = 1.7993, num_step_acc = 0.3729, eval time = 19.68s\n",
      "Step 5000, Data test prior_loss = 44.0021, loss = 22.7809, latent_code_prior_loss = 39.1710, imp_weight = 0.4181, num_steps_prior_loss = 4.8311, l2_loss = 0.1057, baseline_loss = 452.0204, reinforce_loss = -5.9603, rec_loss = 18.8207, num_step = 1.7615, num_step_acc = 0.3760, eval time = 0.3617s\n",
      "\n",
      "Step 6000, Data train prior_loss = 48.3496, loss = 22.8925, latent_code_prior_loss = 43.3001, imp_weight = -0.2533, num_steps_prior_loss = 5.0495, l2_loss = 0.1160, baseline_loss = 428.0553, reinforce_loss = -6.3490, rec_loss = 18.5410, num_step = 1.8296, num_step_acc = 0.3549, eval time = 19.53s\n",
      "Step 6000, Data test prior_loss = 46.2998, loss = 21.2462, latent_code_prior_loss = 41.3605, imp_weight = -1.8553, num_steps_prior_loss = 4.9393, l2_loss = 0.1160, baseline_loss = 375.0844, reinforce_loss = -5.5338, rec_loss = 17.0793, num_step = 1.7885, num_step_acc = 0.3823, eval time = 0.3127s\n",
      "\n",
      "Step 7000, Data train prior_loss = 49.6230, loss = 22.1987, latent_code_prior_loss = 44.7114, imp_weight = -0.0718, num_steps_prior_loss = 4.9116, l2_loss = 0.1250, baseline_loss = 409.5175, reinforce_loss = -6.2932, rec_loss = 17.7327, num_step = 1.7843, num_step_acc = 0.3670, eval time = 19.42s\n",
      "Step 7000, Data test prior_loss = 48.4270, loss = 20.9806, latent_code_prior_loss = 43.5773, imp_weight = -1.2332, num_steps_prior_loss = 4.8497, l2_loss = 0.1250, baseline_loss = 374.0358, reinforce_loss = -4.9548, rec_loss = 16.6222, num_step = 1.7729, num_step_acc = 0.3625, eval time = 0.3199s\n",
      "\n",
      "Step 8000, Data train prior_loss = 50.0526, loss = 21.6398, latent_code_prior_loss = 45.0434, imp_weight = -0.6710, num_steps_prior_loss = 5.0092, l2_loss = 0.1289, baseline_loss = 390.6532, reinforce_loss = -5.9418, rec_loss = 17.1351, num_step = 1.8163, num_step_acc = 0.3617, eval time = 19.45s\n",
      "Step 8000, Data test prior_loss = 49.0582, loss = 20.3424, latent_code_prior_loss = 44.2047, imp_weight = -1.9960, num_steps_prior_loss = 4.8536, l2_loss = 0.1289, baseline_loss = 350.3978, reinforce_loss = -5.0200, rec_loss = 15.9272, num_step = 1.7552, num_step_acc = 0.3760, eval time = 0.3132s\n",
      "\n",
      "Step 9000, Data train prior_loss = 47.5030, loss = 20.4766, latent_code_prior_loss = 40.0780, imp_weight = 0.5701, num_steps_prior_loss = 7.4250, l2_loss = 0.1332, baseline_loss = 354.2442, reinforce_loss = -4.0686, rec_loss = 16.2013, num_step = 2.6710, num_step_acc = 0.0627, eval time = 19.62s\n",
      "Step 9000, Data test prior_loss = 45.5907, loss = 19.1020, latent_code_prior_loss = 38.2584, imp_weight = -0.7728, num_steps_prior_loss = 7.3323, l2_loss = 0.1332, baseline_loss = 319.1828, reinforce_loss = -4.4949, rec_loss = 14.9988, num_step = 2.6219, num_step_acc = 0.0594, eval time = 0.3106s\n",
      "\n",
      "Step 10000, Data train prior_loss = 50.4998, loss = 21.0469, latent_code_prior_loss = 45.6599, imp_weight = 1.4274, num_steps_prior_loss = 4.8399, l2_loss = 0.1376, baseline_loss = 372.6738, reinforce_loss = -7.2389, rec_loss = 16.5019, num_step = 1.7659, num_step_acc = 0.3718, eval time = 19.47s\n",
      "Step 10000, Data test prior_loss = 48.5009, loss = 19.7661, latent_code_prior_loss = 43.8800, imp_weight = 0.1588, num_steps_prior_loss = 4.6209, l2_loss = 0.1376, baseline_loss = 328.7323, reinforce_loss = -5.7615, rec_loss = 15.4010, num_step = 1.6885, num_step_acc = 0.3875, eval time = 0.3133s\n",
      "\n",
      "Step 11000, Data train prior_loss = 49.8415, loss = 19.8031, latent_code_prior_loss = 42.4017, imp_weight = 0.1197, num_steps_prior_loss = 7.4399, l2_loss = 0.1425, baseline_loss = 330.5671, reinforce_loss = -4.2868, rec_loss = 15.3174, num_step = 2.6812, num_step_acc = 0.0552, eval time = 19.43s\n",
      "Step 11000, Data test prior_loss = 47.6593, loss = 18.6221, latent_code_prior_loss = 40.4362, imp_weight = -1.0945, num_steps_prior_loss = 7.2231, l2_loss = 0.1425, baseline_loss = 290.2585, reinforce_loss = -3.7697, rec_loss = 14.3327, num_step = 2.5719, num_step_acc = 0.0906, eval time = 0.3118s\n",
      "\n",
      "Step 12000, Data train prior_loss = 49.2349, loss = 19.4991, latent_code_prior_loss = 41.7528, imp_weight = -0.0910, num_steps_prior_loss = 7.4821, l2_loss = 0.1461, baseline_loss = 322.2871, reinforce_loss = -3.8626, rec_loss = 15.0679, num_step = 2.6895, num_step_acc = 0.0530, eval time = 19.49s\n",
      "Step 12000, Data test prior_loss = 47.6309, loss = 18.3879, latent_code_prior_loss = 40.2988, imp_weight = -1.1742, num_steps_prior_loss = 7.3321, l2_loss = 0.1461, baseline_loss = 308.8511, reinforce_loss = -3.9581, rec_loss = 14.1011, num_step = 2.6542, num_step_acc = 0.0615, eval time = 0.3228s\n",
      "\n",
      "Step 13000, Data train prior_loss = 48.4030, loss = 19.3481, latent_code_prior_loss = 40.9516, imp_weight = 0.4485, num_steps_prior_loss = 7.4514, l2_loss = 0.1507, baseline_loss = 316.7629, reinforce_loss = -4.0834, rec_loss = 14.9919, num_step = 2.6815, num_step_acc = 0.0636, eval time = 19.42s\n",
      "Step 13000, Data test prior_loss = 47.7430, loss = 17.4233, latent_code_prior_loss = 40.3351, imp_weight = -1.3913, num_steps_prior_loss = 7.4079, l2_loss = 0.1507, baseline_loss = 267.6576, reinforce_loss = -2.0547, rec_loss = 13.1265, num_step = 2.7021, num_step_acc = 0.0510, eval time = 0.3139s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14000, Data train prior_loss = 48.7672, loss = 19.2100, latent_code_prior_loss = 41.3661, imp_weight = 0.7043, num_steps_prior_loss = 7.4010, l2_loss = 0.1543, baseline_loss = 320.9741, reinforce_loss = -4.6373, rec_loss = 14.8210, num_step = 2.6619, num_step_acc = 0.0846, eval time = 19.33s\n",
      "Step 14000, Data test prior_loss = 48.1060, loss = 17.9421, latent_code_prior_loss = 40.7005, imp_weight = -0.5198, num_steps_prior_loss = 7.4056, l2_loss = 0.1543, baseline_loss = 285.9125, reinforce_loss = -3.9035, rec_loss = 13.6125, num_step = 2.6781, num_step_acc = 0.0646, eval time = 0.3204s\n",
      "\n",
      "Step 15000, Data train prior_loss = 52.4923, loss = 20.0496, latent_code_prior_loss = 47.6329, imp_weight = 0.5385, num_steps_prior_loss = 4.8595, l2_loss = 0.1570, baseline_loss = 341.5709, reinforce_loss = -6.6925, rec_loss = 15.3253, num_step = 1.7711, num_step_acc = 0.3699, eval time = 19.55s\n",
      "Step 15000, Data test prior_loss = 51.5333, loss = 18.9491, latent_code_prior_loss = 46.8996, imp_weight = -0.5317, num_steps_prior_loss = 4.6338, l2_loss = 0.1570, baseline_loss = 301.9385, reinforce_loss = -5.5960, rec_loss = 14.3111, num_step = 1.7094, num_step_acc = 0.3906, eval time = 0.3176s\n",
      "\n",
      "Step 16000, Data train prior_loss = 56.1978, loss = 19.9671, latent_code_prior_loss = 51.3421, imp_weight = -0.2989, num_steps_prior_loss = 4.8556, l2_loss = 0.1605, baseline_loss = 340.7026, reinforce_loss = -6.1237, rec_loss = 14.9093, num_step = 1.7633, num_step_acc = 0.3725, eval time = 19.51s\n",
      "Step 16000, Data test prior_loss = 55.2352, loss = 18.6678, latent_code_prior_loss = 50.5466, imp_weight = -1.5832, num_steps_prior_loss = 4.6886, l2_loss = 0.1605, baseline_loss = 302.6021, reinforce_loss = -5.3547, rec_loss = 13.6967, num_step = 1.7104, num_step_acc = 0.3865, eval time = 0.3302s\n",
      "\n",
      "Step 17000, Data train prior_loss = 48.9428, loss = 18.9368, latent_code_prior_loss = 41.5422, imp_weight = 0.8468, num_steps_prior_loss = 7.4006, l2_loss = 0.1626, baseline_loss = 318.4099, reinforce_loss = -4.8579, rec_loss = 14.5320, num_step = 2.6659, num_step_acc = 0.0682, eval time = 19.49s\n",
      "Step 17000, Data test prior_loss = 47.0658, loss = 17.5978, latent_code_prior_loss = 39.8171, imp_weight = -0.4588, num_steps_prior_loss = 7.2487, l2_loss = 0.1626, baseline_loss = 294.6319, reinforce_loss = -4.4229, rec_loss = 13.3618, num_step = 2.6385, num_step_acc = 0.0771, eval time = 0.3137s\n",
      "\n",
      "Step 18000, Data train prior_loss = 52.5589, loss = 19.6278, latent_code_prior_loss = 47.6222, imp_weight = 0.8679, num_steps_prior_loss = 4.9367, l2_loss = 0.1631, baseline_loss = 336.1402, reinforce_loss = -6.6611, rec_loss = 14.8975, num_step = 1.7921, num_step_acc = 0.3587, eval time = 19.31s\n",
      "Step 18000, Data test prior_loss = 51.3508, loss = 18.5296, latent_code_prior_loss = 46.5465, imp_weight = -0.2707, num_steps_prior_loss = 4.8043, l2_loss = 0.1631, baseline_loss = 292.4960, reinforce_loss = -5.9584, rec_loss = 13.9080, num_step = 1.7417, num_step_acc = 0.3760, eval time = 0.3177s\n",
      "\n",
      "Step 19000, Data train prior_loss = 53.5248, loss = 19.5503, latent_code_prior_loss = 48.6748, imp_weight = -0.1127, num_steps_prior_loss = 4.8500, l2_loss = 0.1639, baseline_loss = 331.4921, reinforce_loss = -6.4089, rec_loss = 14.7331, num_step = 1.7674, num_step_acc = 0.3698, eval time = 19.52s\n",
      "Step 19000, Data test prior_loss = 52.0943, loss = 18.5148, latent_code_prior_loss = 47.4065, imp_weight = -1.1526, num_steps_prior_loss = 4.6879, l2_loss = 0.1639, baseline_loss = 301.6103, reinforce_loss = -6.0109, rec_loss = 13.8263, num_step = 1.7167, num_step_acc = 0.3802, eval time = 0.3218s\n",
      "\n",
      "Step 20000, Data train prior_loss = 54.1702, loss = 19.5884, latent_code_prior_loss = 49.2755, imp_weight = -1.4791, num_steps_prior_loss = 4.8947, l2_loss = 0.1660, baseline_loss = 329.2563, reinforce_loss = -5.5010, rec_loss = 14.7131, num_step = 1.7787, num_step_acc = 0.3588, eval time = 19.55s\n",
      "Step 20000, Data test prior_loss = 52.1306, loss = 17.6741, latent_code_prior_loss = 47.4300, imp_weight = -3.3652, num_steps_prior_loss = 4.7006, l2_loss = 0.1660, baseline_loss = 275.8191, reinforce_loss = -2.9969, rec_loss = 12.9824, num_step = 1.7563, num_step_acc = 0.3719, eval time = 0.3304s\n",
      "\n",
      "Step 21000, Data train prior_loss = 53.6584, loss = 19.5067, latent_code_prior_loss = 48.8096, imp_weight = -0.6132, num_steps_prior_loss = 4.8487, l2_loss = 0.1666, baseline_loss = 326.9412, reinforce_loss = -5.7592, rec_loss = 14.6775, num_step = 1.7670, num_step_acc = 0.3678, eval time = 19.62s\n",
      "Step 21000, Data test prior_loss = 51.1616, loss = 18.2197, latent_code_prior_loss = 46.4268, imp_weight = -1.8391, num_steps_prior_loss = 4.7348, l2_loss = 0.1666, baseline_loss = 285.9547, reinforce_loss = -5.2130, rec_loss = 13.6152, num_step = 1.7031, num_step_acc = 0.3781, eval time = 0.3226s\n",
      "\n",
      "Step 22000, Data train prior_loss = 53.1373, loss = 19.4658, latent_code_prior_loss = 48.3428, imp_weight = -0.0029, num_steps_prior_loss = 4.7945, l2_loss = 0.1671, baseline_loss = 324.3538, reinforce_loss = -6.1840, rec_loss = 14.6834, num_step = 1.7523, num_step_acc = 0.3672, eval time = 19.81s\n",
      "Step 22000, Data test prior_loss = 50.4969, loss = 18.1972, latent_code_prior_loss = 45.8717, imp_weight = -1.1985, num_steps_prior_loss = 4.6252, l2_loss = 0.1671, baseline_loss = 299.5914, reinforce_loss = -5.6522, rec_loss = 13.6525, num_step = 1.6979, num_step_acc = 0.3927, eval time = 0.3156s\n",
      "\n",
      "Step 23000, Data train prior_loss = 52.5618, loss = 19.4627, latent_code_prior_loss = 47.6386, imp_weight = -0.3641, num_steps_prior_loss = 4.9232, l2_loss = 0.1666, baseline_loss = 325.8250, reinforce_loss = -6.0345, rec_loss = 14.7322, num_step = 1.7881, num_step_acc = 0.3664, eval time = 19.34s\n",
      "Step 23000, Data test prior_loss = 50.8191, loss = 18.3431, latent_code_prior_loss = 45.9356, imp_weight = -1.4602, num_steps_prior_loss = 4.8835, l2_loss = 0.1666, baseline_loss = 312.2827, reinforce_loss = -6.0940, rec_loss = 13.7694, num_step = 1.7750, num_step_acc = 0.3604, eval time = 0.3229s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c9337160ca7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_itr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_itr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_itr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print 'Starting training at iter = {}'.format(train_itr)\n",
    "\n",
    "for train_itr in xrange(train_itr+1, int(1e7)):\n",
    "        \n",
    "    sess.run(train_step)\n",
    "    \n",
    "    if train_itr % 1000 == 0:\n",
    "        summaries = sess.run(all_summaries)\n",
    "        summary_writer.add_summary(summaries, train_itr)\n",
    "        \n",
    "    if train_itr % 1000 == 0:\n",
    "        log(train_itr)\n",
    "        \n",
    "    if train_itr % 1000 == 0:\n",
    "#         saver.save(sess, checkpoint_name, global_step=train_itr)\n",
    "        make_fig(logdir, train_itr)    \n",
    "    \n",
    "#     if train_itr % 1000 == 0:\n",
    "#         print 'baseline value = {}'.format(sess.run(baseline_mean))\n",
    "#         print_grad_variance()3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
